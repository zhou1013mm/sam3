{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SAM 3 Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook shows an example of how an MLLM can use SAM 3 as a tool, i.e., \"SAM 3 Agent\", to segment more complex text queries such as \"the leftmost child wearing blue vest\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Env Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First install `sam3` in your environment using the [installation instructions](https://github.com/facebookresearch/sam3?tab=readme-ov-file#installation) in the repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "# turn on tfloat32 for Ampere GPUs\n",
        "# https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# use bfloat16 for the entire notebook. If your card doesn't support it, try float16 instead\n",
        "torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "# inference mode for the whole notebook. Disable if you need gradients\n",
        "torch.inference_mode().__enter__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Feb  6 08:26:16 2026       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.274.02             Driver Version: 535.274.02   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA RTX A5000               Off | 00000000:3D:00.0 Off |                  Off |\n",
            "| 30%   50C    P2             201W / 230W |  17774MiB / 24564MiB |     89%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   1  NVIDIA RTX A5000               Off | 00000000:3E:00.0 Off |                  Off |\n",
            "| 30%   50C    P2             205W / 230W |  21788MiB / 24564MiB |     90%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   2  NVIDIA RTX A5000               Off | 00000000:40:00.0 Off |                  Off |\n",
            "| 30%   50C    P2             198W / 230W |  17752MiB / 24564MiB |     77%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   3  NVIDIA RTX A5000               Off | 00000000:41:00.0 Off |                  Off |\n",
            "| 30%   49C    P2             207W / 230W |  17716MiB / 24564MiB |     91%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   4  NVIDIA RTX A5000               Off | 00000000:B1:00.0 Off |                  Off |\n",
            "| 30%   49C    P2             218W / 230W |  21131MiB / 24564MiB |    100%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   5  NVIDIA RTX A5000               Off | 00000000:B2:00.0 Off |                  Off |\n",
            "| 30%   50C    P2             218W / 230W |  20906MiB / 24564MiB |    100%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   6  NVIDIA RTX A5000               Off | 00000000:B4:00.0 Off |                  Off |\n",
            "| 30%   23C    P8              19W / 230W |  20498MiB / 24564MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   7  NVIDIA RTX A5000               Off | 00000000:B5:00.0 Off |                  Off |\n",
            "| 30%   24C    P8              17W / 230W |  20498MiB / 24564MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A   2208412      C   ...conda3/envs/reconvla/bin/python3.10    17768MiB |\n",
            "|    1   N/A  N/A   2208413      C   ...conda3/envs/reconvla/bin/python3.10    21782MiB |\n",
            "|    2   N/A  N/A   2208414      C   ...conda3/envs/reconvla/bin/python3.10    17746MiB |\n",
            "|    3   N/A  N/A   2208415      C   ...conda3/envs/reconvla/bin/python3.10    17710MiB |\n",
            "|    4   N/A  N/A   3515805      C   ...ojia/miniconda3/envs/uwm/bin/python    20884MiB |\n",
            "|    4   N/A  N/A   3515806      C   ...ojia/miniconda3/envs/uwm/bin/python      204MiB |\n",
            "|    5   N/A  N/A   3515806      C   ...ojia/miniconda3/envs/uwm/bin/python    20884MiB |\n",
            "|    6   N/A  N/A   3513223      C   VLLM::Worker_TP0                          20492MiB |\n",
            "|    7   N/A  N/A   3513224      C   VLLM::Worker_TP1                          20492MiB |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "SAM3_ROOT = os.path.dirname(os.getcwd())\n",
        "os.chdir(SAM3_ROOT)\n",
        "\n",
        "# setup GPU to use -  A single GPU is good with the purpose of this demo\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
        "_ = os.system(\"nvidia-smi\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build SAM3 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data0/guojia/work_huang/sam3/sam3/model_builder.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "import sam3\n",
        "from sam3 import build_sam3_image_model\n",
        "from sam3.model.sam3_image_processor import Sam3Processor\n",
        "\n",
        "sam3_root = os.path.dirname(sam3.__file__)\n",
        "bpe_path = f\"{sam3_root}/assets/bpe_simple_vocab_16e6.txt.gz\"\n",
        "checkpoint_path = \"/data0/guojia/.cache/modelscope/models/facebook/sam3/sam3.pt\"\n",
        "model = build_sam3_image_model(bpe_path=bpe_path,checkpoint_path=checkpoint_path)\n",
        "processor = Sam3Processor(model, confidence_threshold=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM Setup\n",
        "\n",
        "Config which MLLM to use, it can either be a model served by vLLM that you launch from your own machine or a model is served via external API. If you want to using a vLLM model, we also provided insturctions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "LLM_CONFIGS = {\n",
        "    # vLLM-served models\n",
        "    \"qwen3_vl_8b_thinking\": {\n",
        "        \"provider\": \"vllm\",\n",
        "        \"model\": \"Qwen/Qwen3-VL-8B-Thinking\",\n",
        "    },\n",
        "    # models served via external APIs\n",
        "    # add your own\n",
        "}\n",
        "\n",
        "model = \"qwen3_vl_8b_thinking\"\n",
        "LLM_API_KEY = \"DUMMY_API_KEY\"\n",
        "\n",
        "llm_config = LLM_CONFIGS[model]\n",
        "llm_config[\"api_key\"] = LLM_API_KEY\n",
        "llm_config[\"name\"] = model\n",
        "\n",
        "# setup API endpoint\n",
        "if llm_config[\"provider\"] == \"vllm\":\n",
        "    LLM_SERVER_URL = \"http://localhost:8000/v1\"  # replace this with your vLLM server address as needed\n",
        "else:\n",
        "    LLM_SERVER_URL = llm_config[\"base_url\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup vLLM server \n",
        "This step is only required if you are using a model served by vLLM, skip this step if you are calling LLM using an API like Gemini and GPT.\n",
        "\n",
        "* Install vLLM (in a separate conda env from SAM 3 to avoid dependency conflicts).\n",
        "  ```bash\n",
        "    conda create -n vllm python=3.12\n",
        "    pip install vllm --extra-index-url https://download.pytorch.org/whl/cu128\n",
        "  ```\n",
        "* Start vLLM server on the same machine of this notebook\n",
        "  ```bash\n",
        "    # qwen 3 VL 8B thinking\n",
        "    vllm serve Qwen/Qwen3-VL-8B-Thinking --tensor-parallel-size 4 --allowed-local-media-path / --enforce-eager --port 8001\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run SAM3 Agent Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data0/guojia/work_huang/sam3/sam3/agent/agent_core.py:205: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
            "  assert (\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "from IPython.display import display, Image\n",
        "from sam3.agent.client_llm import send_generate_request as send_generate_request_orig\n",
        "from sam3.agent.client_sam3 import call_sam_service as call_sam_service_orig\n",
        "from sam3.agent.inference import run_single_image_inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 689664053567678,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ Starting SAM 3 Agent Session... ------------------------------ \n",
            "> Text prompt: yellow fish\n",
            "> Image path: /data0/guojia/work_huang/sam3/fish.jpg\n",
            "\n",
            "\n",
            "\n",
            "------------------------------ Round 1------------------------------\n",
            "\n",
            "\n",
            "\n",
            "image_path /data0/guojia/work_huang/sam3/fish.jpg\n",
            "ðŸ” Calling model Qwen/Qwen3-VL-8B-Thinking...\n",
            "Request failed: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 16384 tokens and your request has 14041 input tokens (4096 > 16384 - 14041). (parameter=max_tokens, value=4096)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\n",
            "\n",
            ">>> MLLM Response [start]\n",
            "None\n",
            "<<< MLLM Response [end]\n",
            "\n",
            "\n",
            "\n",
            ">>> SAM 3 Agent execution ended.\n",
            "\n",
            "\n",
            "Saved messages history that caused error to: agent_output/none_out/fish_error_history.json\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Generated text is None, which is unexpected. Please check the Qwen server and the input parameters for image path: /data0/guojia/work_huang/sam3/fish.jpg and initial text prompt: yellow fish.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m send_generate_request = partial(send_generate_request_orig, server_url=LLM_SERVER_URL, model=llm_config[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], api_key=llm_config[\u001b[33m\"\u001b[39m\u001b[33mapi_key\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      6\u001b[39m call_sam_service = partial(call_sam_service_orig, sam3_processor=processor)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m output_image_path = \u001b[43mrun_single_image_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msend_generate_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_sam_service\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magent_output\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# display output\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_image_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/data0/guojia/work_huang/sam3/sam3/agent/inference.py:45\u001b[39m, in \u001b[36mrun_single_image_inference\u001b[39m\u001b[34m(image_path, text_prompt, llm_config, send_generate_request, call_sam_service, output_dir, debug)\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m30\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Starting SAM 3 Agent Session... \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m30\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m agent_history, final_output_dict, rendered_final_output = \u001b[43magent_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43msend_generate_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43msend_generate_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcall_sam_service\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcall_sam_service\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m30\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m End of SAM 3 Agent Session... \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m30\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m final_output_dict[\u001b[33m\"\u001b[39m\u001b[33mtext_prompt\u001b[39m\u001b[33m\"\u001b[39m] = text_prompt\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/data0/guojia/work_huang/sam3/sam3/agent/agent_core.py:563\u001b[39m, in \u001b[36magent_inference\u001b[39m\u001b[34m(img_path, initial_text_prompt, debug, send_generate_request, call_sam_service, max_generations, output_dir)\u001b[39m\n\u001b[32m    561\u001b[39m     json.dump(messages, f, indent=\u001b[32m4\u001b[39m)\n\u001b[32m    562\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved messages history that caused error to:\u001b[39m\u001b[33m\"\u001b[39m, error_save_path)\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    564\u001b[39m     \u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated text is None, which is unexpected. Please check the Qwen server and the input parameters for image path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and initial text prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_text_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    565\u001b[39m )\n",
            "\u001b[31mValueError\u001b[39m: Generated text is None, which is unexpected. Please check the Qwen server and the input parameters for image path: /data0/guojia/work_huang/sam3/fish.jpg and initial text prompt: yellow fish."
          ]
        }
      ],
      "source": [
        "# prepare input args and run single image inference\n",
        "image = \"fish.jpg\"\n",
        "prompt = \"all the yellow fish\"\n",
        "image = os.path.abspath(image)\n",
        "send_generate_request = partial(send_generate_request_orig, server_url=LLM_SERVER_URL, model=llm_config[\"model\"], api_key=llm_config[\"api_key\"])\n",
        "call_sam_service = partial(call_sam_service_orig, sam3_processor=processor)\n",
        "output_image_path = run_single_image_inference(\n",
        "    image, prompt, llm_config, send_generate_request, call_sam_service,\n",
        "    debug=True, output_dir=\"agent_output\"\n",
        ")\n",
        "\n",
        "# display output\n",
        "if output_image_path is not None:\n",
        "    display(Image(filename=output_image_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "be59e249-6c09-4634-a9e7-1f06fd233c42",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "sam3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
